{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENuBhp-cDx0a"
      },
      "outputs": [],
      "source": [
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNNvm_gjGztW"
      },
      "outputs": [],
      "source": [
        "from pprint import pprint\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, BertModel, BertPreTrainedModel\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from itertools import combinations\n",
        "import random\n",
        "from torch.optim import Adam\n",
        "import os\n",
        "from torch import tensor\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o1n6X6NYM7Na"
      },
      "outputs": [],
      "source": [
        "class OntoNotesDataset(Dataset):\n",
        "    def __init__(self, split='train', tokenizer_name='bert-base-uncased', max_length=512):\n",
        "        self.dataset = load_dataset('conll2012_ontonotesv5', 'english_v12', split=split)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "        self.max_length = max_length\n",
        "        self.pos_tags, self.pairs = self.prepare_pairs()\n",
        "\n",
        "    def prepare_pairs(self):\n",
        "        pairs = []\n",
        "        all_pos_tags = []\n",
        "        total_coreference_spans = 0\n",
        "        entity_count = 0\n",
        "        for item in self.dataset:\n",
        "            document = item['sentences']\n",
        "            coreference_spans = {}\n",
        "            coreference_pos_tags_spans = {}\n",
        "            sentences_pos_tags = {}\n",
        "            sentences = {}\n",
        "            sentence_id = 1\n",
        "            for sentence in document:\n",
        "                words = sentence['words']\n",
        "                coref_spans = sentence['coref_spans']\n",
        "                pos = sentence['pos_tags']\n",
        "                for span in coref_spans:\n",
        "                    entity_id, start, end = span\n",
        "                    if entity_id not in coreference_spans:\n",
        "                        coreference_spans[entity_id] = []\n",
        "                        coreference_pos_tags_spans[entity_id] = []\n",
        "                    coreference_spans[entity_id].append([sentence_id, words[start:end+1]])\n",
        "                    coreference_pos_tags_spans[entity_id].append([sentence_id, pos[start:end+1]])\n",
        "                sentences_pos_tags[sentence_id] = pos\n",
        "                sentences[sentence_id] = words\n",
        "                sentence_id += 1\n",
        "\n",
        "            entity_count += len(coreference_spans.keys())\n",
        "            for entity_id in coreference_spans.keys():\n",
        "                spans_of_entity = coreference_spans[entity_id]\n",
        "                pos_tags_of_entity = coreference_pos_tags_spans[entity_id]\n",
        "\n",
        "                n = len(spans_of_entity)\n",
        "                if n % 2 != 0:\n",
        "                    n -= 1\n",
        "                for i in range(0, n, 2):\n",
        "                    span1 = spans_of_entity[i]\n",
        "                    span2 = spans_of_entity[i + 1]\n",
        "                    sentence1 = sentences[span1[0]]\n",
        "                    words_sentence_1 = span1[1]\n",
        "                    sentence2 = sentences[span2[0]]\n",
        "                    words_sentence_2 = span2[1]\n",
        "                    pos_span1 = pos_tags_of_entity[i]\n",
        "                    pos_span2 = pos_tags_of_entity[i + 1]\n",
        "                    pos_sentence1 = sentences_pos_tags[span1[0]]\n",
        "                    pos_words_sentence1 = pos_span1[1]\n",
        "                    pos_sentence2 = sentences_pos_tags[span2[0]]\n",
        "                    pos_words_sentence2 = pos_span2[1]\n",
        "                    pairs.append([sentence1, words_sentence_1, sentence2, words_sentence_2, 1])\n",
        "                    all_pos_tags.append([pos_sentence1, pos_words_sentence1, pos_sentence2, pos_words_sentence2, 1])\n",
        "                    total_coreference_spans += 1\n",
        "\n",
        "                false_entities = [eid for eid in coreference_spans.keys() if eid != entity_id]\n",
        "                if (len(false_entities) > 0):\n",
        "                  for i in range(0, n, 2):\n",
        "                      random_entity = random.choice(false_entities)\n",
        "                      random_span_index_of_entity = random.randint(0, len(coreference_spans[random_entity])-1)\n",
        "                      span1 = spans_of_entity[i]\n",
        "                      span2 = coreference_spans[random_entity][random_span_index_of_entity]\n",
        "                      sentence1 = sentences[span1[0]]\n",
        "                      words_sentence_1 = span1[1]\n",
        "                      sentence2 = sentences[span2[0]]\n",
        "                      words_sentence_2 = span2[1]\n",
        "                      pos_span1 = pos_tags_of_entity[i]\n",
        "                      pos_span2 = coreference_pos_tags_spans[random_entity][random_span_index_of_entity]\n",
        "                      pos_sentence1 = sentences_pos_tags[span1[0]]\n",
        "                      pos_words_sentence1 = pos_span1[1]\n",
        "                      pos_sentence2 = sentences_pos_tags[span2[0]]\n",
        "                      pos_words_sentence2 = pos_span2[1]\n",
        "                      pairs.append([sentence1, words_sentence_1, sentence2, words_sentence_2, 0])\n",
        "                      all_pos_tags.append([pos_sentence1, pos_words_sentence1, pos_sentence2, pos_words_sentence2, 0])\n",
        "\n",
        "        return all_pos_tags, pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        two_sentence = self.pairs[idx]\n",
        "        sentence_one = two_sentence[0]\n",
        "        word_one = two_sentence[1]\n",
        "        sentence_two = two_sentence[2]\n",
        "        word_two = two_sentence[3]\n",
        "        label = two_sentence[4]\n",
        "        sentence_one_text = ' '.join(sentence_one)\n",
        "        sentence_two_text = ' '.join(sentence_two)\n",
        "        word_one_text = ' '.join(word_one)\n",
        "        word_two_text = ' '.join(word_two)\n",
        "\n",
        "        two_pos_tags = self.pos_tags[idx]\n",
        "        sentence_one_pos_tags = two_pos_tags[0]\n",
        "        word_one_pos_tags = two_pos_tags[1]\n",
        "        sentence_two_pos_tags = two_pos_tags[2]\n",
        "        word_two_pos_tags = two_pos_tags[3]\n",
        "\n",
        "        input_sequence = f\"[CLS] {sentence_one_text} [SEP] {sentence_two_text} [SEP] {word_one_text} [SEP] {word_two_text} [SEP]\"\n",
        "        pos_tag_sequence = [0] + sentence_one_pos_tags + [0] + sentence_two_pos_tags + [0] + word_one_pos_tags + [0] + word_two_pos_tags + [0]\n",
        "        pos_tag_sequence.extend([0] * (512 - len(pos_tag_sequence)))\n",
        "        tokenized_inputs = self.tokenizer(input_sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        return {\n",
        "            'input_ids': tokenized_inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokenized_inputs['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long),\n",
        "            'pos_tags': torch.tensor(pos_tag_sequence, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    pos_tags = [item['pos_tags'] for item in batch]\n",
        "\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "    pos_tags_padded = pad_sequence(pos_tags, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids_padded,\n",
        "        'attention_mask': attention_masks_padded,\n",
        "        'label': labels,\n",
        "        'pos_tags': pos_tags_padded,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8OlO2I0k0tq"
      },
      "outputs": [],
      "source": [
        "class OntoNotesDatasetTest(Dataset):\n",
        "    def __init__(self, split='test', tokenizer_name='bert-base-uncased', max_length=512):\n",
        "        self.dataset = load_dataset('conll2012_ontonotesv5', 'english_v12', split=split)\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(tokenizer_name, use_fast=True)\n",
        "        self.max_length = max_length\n",
        "        self.pos_tags, self.pairs, self.combination_lengths = self.prepare_pairs()\n",
        "\n",
        "    def prepare_pairs(self):\n",
        "\n",
        "        pairs = []\n",
        "        all_pos_tags = []\n",
        "        entity_count = 0\n",
        "        combination_lengths = []\n",
        "        for item in self.dataset:\n",
        "            document = item['sentences']\n",
        "            coreference_spans = {}\n",
        "            coreference_pos_tags_spans = {}\n",
        "            sentences_pos_tags = {}\n",
        "            sentences = {}\n",
        "            sentence_id = 1\n",
        "            for sentence in document:\n",
        "                words = sentence['words']\n",
        "                coref_spans = sentence['coref_spans']\n",
        "                pos = sentence['pos_tags']\n",
        "                for span in coref_spans:\n",
        "                    entity_id, start, end = span\n",
        "                    if entity_id not in coreference_spans:\n",
        "                        coreference_spans[entity_id] = []\n",
        "                        coreference_pos_tags_spans[entity_id] = []\n",
        "                    coreference_spans[entity_id].append([sentence_id, words[start:end+1]])\n",
        "                    coreference_pos_tags_spans[entity_id].append([sentence_id, pos[start:end+1]])\n",
        "                sentences_pos_tags[sentence_id] = pos\n",
        "                sentences[sentence_id] = words\n",
        "                sentence_id += 1\n",
        "\n",
        "            entity_count += len(coreference_spans.keys())\n",
        "            for entity_id in coreference_spans.keys():\n",
        "                spans_of_entity = coreference_spans[entity_id]\n",
        "                pos_tags_of_entity = coreference_pos_tags_spans[entity_id]\n",
        "\n",
        "                n = len(spans_of_entity)\n",
        "                if n % 2 != 0:\n",
        "                    n -= 1\n",
        "\n",
        "                for i in range(0, n, 2):\n",
        "                    span1 = spans_of_entity[i]\n",
        "                    span2 = spans_of_entity[i + 1]\n",
        "                    sentence1 = sentences[span1[0]]\n",
        "                    words_sentence_1 = span1[1]\n",
        "                    sentence2 = sentences[span2[0]]\n",
        "                    words_sentence_2 = span2[1]\n",
        "\n",
        "                    pos_span1 = pos_tags_of_entity[i]\n",
        "                    pos_span2 = pos_tags_of_entity[i + 1]\n",
        "                    pos_sentence1 = sentences_pos_tags[span1[0]]\n",
        "                    pos_words_sentence1 = pos_span1[1]\n",
        "                    pos_sentence2 = sentences_pos_tags[span2[0]]\n",
        "                    pos_words_sentence2 = pos_span2[1]\n",
        "                    combination_len = 0\n",
        "                    for word in sentence2:\n",
        "                      if word not in words_sentence_2:\n",
        "                        word_index = sentence2.index(word)\n",
        "                        pairs.append([sentence1, words_sentence_1, sentence2, [word], 0])\n",
        "                        all_pos_tags.append([pos_sentence1, pos_words_sentence1, pos_sentence2, [pos_sentence2[word_index]], 0])\n",
        "                        combination_len+=1\n",
        "                    pairs.append([sentence1, words_sentence_1, sentence2, words_sentence_2, 1])\n",
        "                    all_pos_tags.append([pos_sentence1, pos_words_sentence1, pos_sentence2, pos_words_sentence2, 1])\n",
        "                    combination_len+=1\n",
        "                    combination_lengths.append(combination_len)\n",
        "        return all_pos_tags, pairs, combination_lengths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        two_sentence = self.pairs[idx]\n",
        "        sentence_one = two_sentence[0]\n",
        "        word_one = two_sentence[1]\n",
        "        sentence_two = two_sentence[2]\n",
        "        word_two = two_sentence[3]\n",
        "        label = two_sentence[4]\n",
        "        sentence_one_text = ' '.join(sentence_one)\n",
        "        sentence_two_text = ' '.join(sentence_two)\n",
        "        word_one_text = ' '.join(word_one)\n",
        "        word_two_text = ' '.join(word_two)\n",
        "\n",
        "        two_pos_tags = self.pos_tags[idx]\n",
        "        sentence_one_pos_tags = two_pos_tags[0]\n",
        "        word_one_pos_tags = two_pos_tags[1]\n",
        "        sentence_two_pos_tags = two_pos_tags[2]\n",
        "        word_two_pos_tags = two_pos_tags[3]\n",
        "\n",
        "        input_sequence = f\"[CLS] {sentence_one_text} [SEP] {sentence_two_text} [SEP] {word_one_text} [SEP] {word_two_text} [SEP]\"\n",
        "        pos_tag_sequence = [0] + sentence_one_pos_tags + [0] + sentence_two_pos_tags + [0] + word_one_pos_tags + [0] + word_two_pos_tags + [0]\n",
        "        pos_tag_sequence.extend([0] * (512 - len(pos_tag_sequence)))\n",
        "        tokenized_inputs = self.tokenizer(input_sequence, return_tensors='pt', padding='max_length', truncation=True, max_length=self.max_length)\n",
        "        return {\n",
        "            'input_ids': tokenized_inputs['input_ids'].squeeze(0),\n",
        "            'attention_mask': tokenized_inputs['attention_mask'].squeeze(0),\n",
        "            'label': torch.tensor(label, dtype=torch.long),\n",
        "            'pos_tags': torch.tensor(pos_tag_sequence, dtype=torch.long),\n",
        "        }\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = [item['input_ids'] for item in batch]\n",
        "    attention_masks = [item['attention_mask'] for item in batch]\n",
        "    labels = torch.tensor([item['label'] for item in batch])\n",
        "    pos_tags = [item['pos_tags'] for item in batch]\n",
        "\n",
        "    input_ids_padded = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
        "    attention_masks_padded = pad_sequence(attention_masks, batch_first=True, padding_value=0)\n",
        "    pos_tags_padded = pad_sequence(pos_tags, batch_first=True, padding_value=0)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_ids_padded,\n",
        "        'attention_mask': attention_masks_padded,\n",
        "        'label': labels,\n",
        "        'pos_tags': pos_tags_padded,\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RO0R08Hsnicl"
      },
      "outputs": [],
      "source": [
        "class CorefResolver(BertPreTrainedModel):\n",
        "    def __init__(self, config, pos_tag_dim = 50, num_pos_tags = 51):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        self.pos_tag_embeddings = nn.Embedding(num_pos_tags, pos_tag_dim, padding_idx=0)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(config.hidden_dropout_prob),\n",
        "            nn.Linear(config.hidden_size + pos_tag_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(config.hidden_dropout_prob),\n",
        "            nn.Linear(256, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, pos_tags, labels=None):\n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
        "        sequence_output = outputs[0]\n",
        "        pos_embeddings = self.pos_tag_embeddings(pos_tags)\n",
        "        combined = torch.cat((sequence_output, pos_embeddings), dim=-1)\n",
        "        combined = torch.mean(combined, dim=1)\n",
        "        logits = self.classifier(combined).squeeze()\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.BCELoss()\n",
        "            logits = logits.view(-1)\n",
        "            loss = loss_fct(logits, labels.float())\n",
        "        return (loss, logits) if loss is not None else logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOMelFt6s5qp"
      },
      "outputs": [],
      "source": [
        "dataset = OntoNotesDataset(split='train', tokenizer_name='bert-base-uncased', max_length=512)\n",
        "data_loader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "model = CorefResolver.from_pretrained('bert-base-uncased')\n",
        "optimizer = Adam(model.parameters(), lr=1e-5)\n",
        "\n",
        "\n",
        "def train(model, data_loader, optimizer, epochs=5):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        progress_bar = tqdm(data_loader, desc=f\"Epoch {epoch+1}\", leave=False)\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            pos_tags = batch['pos_tags'].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            try:\n",
        "                loss, _ = model(input_ids, attention_mask, pos_tags, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                progress_bar.set_postfix(loss=f\"{total_loss / (progress_bar.last_print_n + 1):.4f}\")\n",
        "            except RuntimeError as e:\n",
        "                print(f\"Error during training: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        avg_loss = total_loss / len(data_loader)\n",
        "        print(f\"Epoch {epoch+1}: Average Loss = {avg_loss:.4f}\")\n",
        "\n",
        "        save_path = 'drive/MyDrive/COMP442FinalProject/models'\n",
        "        if not os.path.exists(save_path):\n",
        "            os.makedirs(save_path)\n",
        "        model_save_path = os.path.join(save_path, f\"conll_pos_tag_embdim50_epoch_{epoch+1}.pt\")\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict()\n",
        "        }, model_save_path)\n",
        "        print(f\"Model saved to {model_save_path}\")\n",
        "\n",
        "\n",
        "train(model, data_loader, optimizer, epochs=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCHV1lcJu1C_"
      },
      "source": [
        "# **Span Index Prediction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JXOGhBQRt0ZE"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    thresholds = [0.6, 0.7, 0.8, 0.9, 0.95]\n",
        "    metrics = {\n",
        "        \"Accuracy\": [],\n",
        "        \"Precision\": [],\n",
        "        \"Recall\": [],\n",
        "        \"F1 Score\": []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for threshold in thresholds:\n",
        "            y_pred = []\n",
        "            y_true = []\n",
        "            a = 0\n",
        "            for batch in tqdm(data_loader, desc=f\"Evaluating for threshold {threshold}\"):\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                pos_tags = batch['pos_tags'].to(device)\n",
        "                labels = batch['label'].to(device)\n",
        "\n",
        "                logits = model(input_ids, attention_mask, pos_tags)\n",
        "                pred_labels = (logits > threshold).cpu().numpy()\n",
        "                y_pred.append(pred_labels)\n",
        "                y_true.append(labels.cpu().numpy())\n",
        "                a+=1\n",
        "                if a == 1000:\n",
        "                  break\n",
        "\n",
        "            accuracy = accuracy_score(y_true, y_pred)\n",
        "            precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "            recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "            f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "            metrics[\"Accuracy\"].append(accuracy)\n",
        "            metrics[\"Precision\"].append(precision)\n",
        "            metrics[\"Recall\"].append(recall)\n",
        "            metrics[\"F1 Score\"].append(f1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for metric, values in metrics.items():\n",
        "        print(metric, values)\n",
        "        plt.plot(thresholds, values, marker='o', label=metric)\n",
        "\n",
        "    plt.title(\"Evaluation Metrics Across Different Thresholds\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "model = CorefResolver.from_pretrained('bert-base-uncased')\n",
        "model_save_path = 'drive/MyDrive/COMP442FinalProject/models/conll_pos_tag_embdim50_epoch_1.pt'\n",
        "checkpoint = torch.load(model_save_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "dataset_test = OntoNotesDatasetTest(split='test', tokenizer_name='bert-base-uncased', max_length=512)\n",
        "combination_lengths = dataset_test.combination_lengths\n",
        "loader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "metrics = evaluate_model(model, loader_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opYkSZIGu3UP"
      },
      "source": [
        "# **Binary classification on mention pairs**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V2HjKdLs1Dq"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, data_loader):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    thresholds = [0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
        "    metrics = {\n",
        "        \"Accuracy\": [],\n",
        "        \"Precision\": [],\n",
        "        \"Recall\": [],\n",
        "        \"F1 Score\": []\n",
        "    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "      for threshold in thresholds:\n",
        "        y_pred = []\n",
        "        y_true = []\n",
        "        a = 0\n",
        "        for batch in tqdm(data_loader, desc=\"Evaluating\"):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "            pos_tags = batch['pos_tags'].to(device)\n",
        "\n",
        "            logits = model(input_ids, attention_mask, pos_tags)\n",
        "\n",
        "            probabilities = torch.sigmoid(logits)\n",
        "            predictions = (probabilities > threshold).float()\n",
        "            y_pred.append(predictions.cpu().numpy())\n",
        "            y_true.append(labels.cpu().numpy())\n",
        "            a+=1\n",
        "            if a == 1000:\n",
        "              break\n",
        "        accuracy = accuracy_score(y_true, y_pred)\n",
        "        precision = precision_score(y_true, y_pred, zero_division=0)\n",
        "        recall = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
        "\n",
        "        metrics[\"Accuracy\"].append(accuracy)\n",
        "        metrics[\"Precision\"].append(precision)\n",
        "        metrics[\"Recall\"].append(recall)\n",
        "        metrics[\"F1 Score\"].append(f1)\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for metric, values in metrics.items():\n",
        "        plt.plot(thresholds, values, marker='o', label=metric)\n",
        "\n",
        "    plt.title(\"Evaluation Metrics Across Different Thresholds\")\n",
        "    plt.xlabel(\"Threshold\")\n",
        "    plt.ylabel(\"Metric Value\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "\n",
        "model = CorefResolver.from_pretrained('bert-base-uncased')\n",
        "model_save_path = 'drive/MyDrive/COMP442FinalProject/models/conll_pos_tag_embdim50_epoch_1.pt'\n",
        "checkpoint = torch.load(model_save_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "dataset_test = OntoNotesDataset(split='test', tokenizer_name='bert-base-uncased', max_length=512)\n",
        "loader_test = DataLoader(dataset_test, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "results = evaluate_model(model, loader_test)\n",
        "print(\"Evaluation Results:\", results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
